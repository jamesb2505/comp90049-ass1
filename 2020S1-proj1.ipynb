{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP90049 Introduction to Machine Learning, 2020 Semester 1\n",
    "-----\n",
    "## Project 1: Understanding Student Success with Naive Bayes\n",
    "-----\n",
    "###### Student Name(s): James Barnes, 820946\n",
    "###### Python version: 3.7.6\n",
    "###### Submission deadline: 11am, Wed 22 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Project 1 submission. \n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas version: 1.0.2\n",
      "Numpy version:  1.18.1\n"
     ]
    }
   ],
   "source": [
    "from sys import version as py_version\n",
    "from collections import Counter, defaultdict as dd\n",
    "from math import log, inf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(f\"Python version: {py_version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version:  {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "def load_data(filename):\n",
    "    return pd.read_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should split a data set into a training set and hold-out test set\n",
    "def split_data(data, class_label, frac=0.5, **kwargs):\n",
    "    tr = data.sample(frac=frac, **kwargs).sort_index()\n",
    "    te = data[~data.index.isin(tr.index)]\n",
    "    \n",
    "    tr_labels = tr[class_label]\n",
    "    te_labels = te[class_label]\n",
    "    \n",
    "    tr = tr.drop(columns=class_label)\n",
    "    te = te.drop(columns=class_label)\n",
    "    \n",
    "    return tr, te, tr_labels, te_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have chosen to implement the NB classifier as a class to keep the namespace clean \n",
    "# for implementing the One R Classifier\n",
    "class NaiveBayesClassifier:\n",
    "    def train(self, data, labels, a=1):\n",
    "        n = data.shape[0]\n",
    "        n_labels = Counter(labels.tolist())\n",
    "        n_attrs = { attr: data[attr].nunique() for attr in data.columns }\n",
    "        denoms = { attr: { label: a * n_attrs[attr] + n_labels[label] \n",
    "                  for label in n_labels } for attr in data.columns } \n",
    "        \n",
    "        priors = { label: count / n for label, count in n_labels.items() }\n",
    "        likelihoods = dd(dict)\n",
    "        for attr in data.columns:\n",
    "            for label in n_labels:\n",
    "                likelihoods[attr][label] = dd(lambda: a / denoms[attr][label])\n",
    "                for val, count in Counter(data[labels == label][attr].tolist()).items():\n",
    "                    likelihoods[attr][label][val] = (a + count) / denoms[attr][label]\n",
    "                    \n",
    "        self.priors = priors\n",
    "        self.likelihoods = likelihoods\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, instances):\n",
    "        predictions = pd.Series(dtype='object')\n",
    "        \n",
    "        for i in instances.index:\n",
    "            instance = list(instances.loc[i].items())\n",
    "            best_label, max_prob = None, -inf\n",
    "            for label, prior in self.priors.items():\n",
    "                prob = log(prior) \\\n",
    "                       + sum(log(self.likelihoods[attr][label][val]) for attr, val in instance)\n",
    "                if prob > max_prob:\n",
    "                    best_label = label\n",
    "                    max_prob = prob\n",
    "            predictions.loc[i] = best_label \n",
    "            \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(actual_labels, predicted_labels):\n",
    "    return sum(actual_labels == predicted_labels) / actual_labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3600\n",
      "accurate\n",
      "   school sex address famsize Pstatus  Medu  Fedu      Mjob      Fjob  reason  \\\n",
      "4      MS   F       R     GT3       T   mid   mid     other     other   other   \n",
      "5      MS   M       U     LE3       T  high  high   teacher    health   other   \n",
      "6      GP   F       R     GT3       T   low   low   at_home     other    home   \n",
      "7      GP   M       U     LE3       T   mid   mid  services    health    home   \n",
      "9      GP   M       U     GT3       T   mid   mid  services   at_home    home   \n",
      "17     MS   M       R     GT3       T  high  high    health     other  course   \n",
      "21     GP   M       U     LE3       T   mid   low   at_home     other  course   \n",
      "33     MS   F       R     GT3       T  high  high     other   teacher   other   \n",
      "64     GP   M       U     LE3       T   mid   low  services     other  course   \n",
      "65     GP   F       R     GT3       T   mid   mid     other     other    home   \n",
      "72     GP   F       U     GT3       T   mid   mid     other     other    home   \n",
      "84     MS   F       R     GT3       T   mid   mid     other     other  course   \n",
      "90     MS   M       U     LE3       A   mid   mid     other  services  course   \n",
      "93     GP   F       U     LE3       T   low   low     other     other    home   \n",
      "99     MS   F       R     GT3       A  high   mid  services  services  course   \n",
      "\n",
      "   guardian traveltime  studytime failures schoolsup famsup paid activities  \\\n",
      "4    mother     medium        low      low        no     no   no         no   \n",
      "5    father        low        low     none        no    yes   no         no   \n",
      "6    mother     medium  very_high     none       yes    yes  yes        yes   \n",
      "7    father        low     medium     none        no    yes   no         no   \n",
      "9    mother        low        low     none        no    yes   no         no   \n",
      "17   father       high        low     high        no     no   no        yes   \n",
      "21   mother        low        low      low        no     no   no        yes   \n",
      "33   father       high     medium     none        no    yes   no         no   \n",
      "64   mother       high     medium      low        no     no   no        yes   \n",
      "65   mother        low     medium     none       yes     no   no         no   \n",
      "72   father        low     medium     none        no    yes   no         no   \n",
      "84   mother       high     medium     none        no    yes   no         no   \n",
      "90   father     medium     medium     none        no    yes   no         no   \n",
      "93   mother     medium     medium     none        no    yes   no         no   \n",
      "99   mother        low        low     none        no    yes   no         no   \n",
      "\n",
      "   nursery higher internet romantic  famrel  freetime  goout  Dalc  Walc  \\\n",
      "4      yes     no      yes      yes       5         5      5     1     1   \n",
      "5      yes    yes      yes       no       4         1      2     2     5   \n",
      "6      yes    yes      yes       no       3         1      2     1     1   \n",
      "7      yes    yes      yes       no       3         2      4     2     4   \n",
      "9      yes     no      yes      yes       4         5      4     1     1   \n",
      "17     yes    yes      yes      yes       3         3      3     1     3   \n",
      "21     yes    yes       no      yes       4         4      4     3     5   \n",
      "33      no    yes      yes      yes       3         2      2     4     2   \n",
      "64      no     no      yes       no       4         4      5     4     4   \n",
      "65     yes    yes       no       no       3         2      3     1     1   \n",
      "72     yes    yes       no       no       4         3      2     1     2   \n",
      "84     yes    yes      yes       no       4         4      5     1     1   \n",
      "90      no    yes      yes      yes       4         1      2     2     2   \n",
      "93      no    yes       no       no       4         4      3     1     1   \n",
      "99     yes    yes      yes       no       5         4      4     3     4   \n",
      "\n",
      "    health       absences Grade  \n",
      "4        3           none     F  \n",
      "5        5           none     C  \n",
      "6        1    four_to_six     C  \n",
      "7        4  more_than_ten     D  \n",
      "9        4    four_to_six     D  \n",
      "17       5   one_to_three     F  \n",
      "21       5    four_to_six     D  \n",
      "33       5           none     F  \n",
      "64       5    four_to_six     D  \n",
      "65       5    four_to_six     C  \n",
      "72       5           none     C  \n",
      "84       4    four_to_six     D  \n",
      "90       5           none     C  \n",
      "93       3   one_to_three     C  \n",
      "99       2  more_than_ten     D  \n",
      "inaccurate\n",
      "   school sex address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
      "0      GP   M       U     GT3       T  high  high  services   teacher   \n",
      "1      MS   M       R     GT3       T   mid   mid     other     other   \n",
      "2      GP   F       R     LE3       T   mid   mid  services  services   \n",
      "11     MS   F       U     GT3       A   mid  high     other  services   \n",
      "12     GP   M       R     LE3       T   low   low   at_home     other   \n",
      "13     MS   F       R     GT3       T   low   low   at_home     other   \n",
      "20     GP   F       R     GT3       T   mid   mid   at_home     other   \n",
      "22     GP   M       U     LE3       T  high  high    health  services   \n",
      "26     MS   F       R     GT3       A   mid   mid    health     other   \n",
      "29     MS   F       U     LE3       A   low   low   at_home  services   \n",
      "32     GP   M       U     LE3       T  high  high   teacher     other   \n",
      "34     GP   M       U     GT3       T   mid   mid     other  services   \n",
      "35     MS   F       R     GT3       T   low   low   at_home  services   \n",
      "36     MS   M       U     GT3       T   low   low   at_home  services   \n",
      "38     GP   F       U     GT3       T   mid   mid    health  services   \n",
      "\n",
      "        reason guardian traveltime studytime failures schoolsup famsup paid  \\\n",
      "0         home   mother        low       low     none        no     no   no   \n",
      "1       course   mother     medium       low     none        no    yes   no   \n",
      "2       course   mother        low      high     none        no    yes   no   \n",
      "11  reputation   father        low    medium      low        no    yes   no   \n",
      "12      course    other     medium    medium   medium        no    yes   no   \n",
      "13      course    other     medium    medium      low        no    yes   no   \n",
      "20  reputation   mother        low       low     none       yes    yes   no   \n",
      "22      course   father        low       low     none        no    yes   no   \n",
      "26      course   mother        low    medium     none        no     no   no   \n",
      "29      course   mother        low    medium     none        no     no   no   \n",
      "32  reputation   mother        low    medium     none        no    yes   no   \n",
      "34  reputation   father        low    medium     none        no     no   no   \n",
      "35       other   mother        low       low      low        no    yes   no   \n",
      "36       other   mother       high    medium     none        no     no   no   \n",
      "38        home   father        low    medium      low        no    yes   no   \n",
      "\n",
      "   activities nursery higher internet romantic  famrel  freetime  goout  Dalc  \\\n",
      "0          no     yes    yes      yes       no       5         2      3     1   \n",
      "1          no      no    yes      yes       no       2         5      5     5   \n",
      "2         yes     yes    yes      yes       no       3         3      2     2   \n",
      "11         no     yes    yes      yes       no       2         3      2     1   \n",
      "12        yes     yes     no      yes      yes       5         3      3     5   \n",
      "13         no     yes    yes      yes      yes       4         3      3     1   \n",
      "20        yes     yes    yes       no       no       4         3      1     1   \n",
      "22        yes     yes    yes      yes       no       4         3      3     1   \n",
      "26         no      no    yes       no      yes       3         3      2     1   \n",
      "29         no     yes    yes       no      yes       5         2      3     1   \n",
      "32        yes     yes    yes      yes       no       4         4      4     1   \n",
      "34         no     yes     no      yes       no       5         5      4     3   \n",
      "35         no     yes    yes       no      yes       4         1      3     1   \n",
      "36         no     yes    yes      yes      yes       5         1      3     3   \n",
      "38         no     yes    yes      yes       no       3         3      2     1   \n",
      "\n",
      "    Walc  health       absences Grade  \n",
      "0      2       5    four_to_six     B  \n",
      "1      5       5  more_than_ten     D  \n",
      "2      2       3           none     D  \n",
      "11     3       1  more_than_ten     F  \n",
      "12     2       4  more_than_ten     D  \n",
      "13     1       3    four_to_six     F  \n",
      "20     1       2  more_than_ten     C  \n",
      "22     3       5           none     C  \n",
      "26     1       3   one_to_three     D  \n",
      "29     2       3   one_to_three     D  \n",
      "32     3       5           none     D  \n",
      "34     5       2  more_than_ten     F  \n",
      "35     1       2    four_to_six     D  \n",
      "36     3       1           none     D  \n",
      "38     1       3   one_to_three     D  \n"
     ]
    }
   ],
   "source": [
    "labels = [\"A+\", \"A\", \"B\", \"C\", \"D\", \"F\"]\n",
    "data = load_data(\"./student.csv\")\n",
    "\n",
    "tr, te, tr_labels, te_labels = split_data(data, \"Grade\", frac=0.5, random_state=3)\n",
    "\n",
    "model = NaiveBayesClassifier().train(tr, tr_labels, a=0.01)\n",
    "predictions = model.predict(te)\n",
    "\n",
    "accuracy = evaluate(te_labels, predictions)\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# # uncomment to see some accurate and inaccurate instances\n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     print(\"accurate\")\n",
    "#     print(data.loc[predictions[predictions == te_labels].index].head(15))\n",
    "\n",
    "#     print(\"inaccurate\")\n",
    "#     print(data.loc[predictions[predictions != te_labels].index].head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "**a)**\n",
    "\n",
    "The naive assumption made in Naive Bayes is that the event of seeing any given attribute is indendent of all other given attributes. \n",
    " \n",
    "This assumption is necessary as it allows us to simplify the expression containing chained conditional probabilities into a product of simpler probabilities, leading to a simpler model. \n",
    " \n",
    "This assumption is unrealistic, as there may be some underlying dependence of attributes, however finding such interdependence is likely too difficult to accurately model generally. This interdependence of variables could be seen with the traveltime and address attributes, where an urban student would be more likely to have a shorter travel time, assuming their school is urban; however, calculating the conditional probability of such events occuring does not seem feasible.\n",
    " \n",
    " \n",
    "**b)**\n",
    "\n",
    "(Implementation above)\n",
    " \n",
    " \n",
    "**c)**\n",
    "\n",
    "The model's accuracy score sits around 35%, depending on the exact split used, as well as the alpha value used in the Laplacian smoothing. This typically varies by around 2%.\n",
    " \n",
    "I was unable to find any strong patterns in both the accurate and inaccurate predictions the model made that were not already present in the other. With a data set with so many features, it can be very hard for a human to analyze and interpret, as there is many variables. This is a primary reason for utilising machine learned models, as they have the ability to learn complex patterns in complex datasets, such as this one.\n",
    " \n",
    " One such example of a pattern that I attempted to find was a relationship between the classes that are correctly predicted when the value of 'Pstatus' is 'A'. In the accurate, these are 'D' and 'C' in the instances I looked at, while in the innacturate these were 'D' and 'F'. Or for the 'internet' feature, where instances have the value 'no', the accurately predicted instances are 'C' and 'D', while the innacurate are also 'C' and 'D'. I do not think that either of these pattern hold any weight in understanding how the classifier makes its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2\n",
    "def precision(vals):\n",
    "    tp, _, fp, _ = vals\n",
    "    return tp / (tp + fp) if tp + fp else 0\n",
    "\n",
    "def recall(vals):\n",
    "    tp, _, _, fn = vals\n",
    "    return tp / (tp + fn) if tp + fn else 0\n",
    "\n",
    "def f1(p, r):\n",
    "    return 2 * p * r / (p + r) if p + r else 0\n",
    "\n",
    "def confusion(actual_labels, predicted_labels, interesting):\n",
    "    actual_labels = actual_labels == interesting\n",
    "    predicted_labels = predicted_labels == interesting\n",
    "    return sum(actual_labels  &  predicted_labels), \\\n",
    "           sum(~actual_labels & ~predicted_labels), \\\n",
    "           sum(~actual_labels &  predicted_labels), \\\n",
    "           sum(actual_labels  & ~predicted_labels)\n",
    "\n",
    "def further_evaluate(actual_labels, predicted_labels, labels):\n",
    "    n = len(labels)\n",
    "    p_m, r_m, label_values = 0, 0, []\n",
    "    for interesting in labels:\n",
    "        vals = confusion(actual_labels, predicted_labels, interesting)\n",
    "        p = precision(vals)\n",
    "        r = recall(vals)\n",
    "        label_values.append((interesting, (p, r, f1(p, r))))\n",
    "        p_m += p; r_m += r\n",
    "    p_m /= n; r_m /= n\n",
    "    label_values.append((\"macro\", (p_m, r_m, f1(p_m, r_m))))\n",
    "    return label_values\n",
    "\n",
    "def pprint_evals(*evals):\n",
    "    for label, (p, r, f) in evals:\n",
    "        print(f\"{label:>5s}: prec = {p:.5f}, rec = {r:.5f}, f1 = {f:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A+: prec = 0.33333, rec = 0.12500, f1 = 0.18182\n",
      "    A: prec = 0.23529, rec = 0.22857, f1 = 0.23188\n",
      "    B: prec = 0.24490, rec = 0.20339, f1 = 0.22222\n",
      "    C: prec = 0.37363, rec = 0.43590, f1 = 0.40237\n",
      "    D: prec = 0.45161, rec = 0.42000, f1 = 0.43523\n",
      "    F: prec = 0.36364, rec = 0.44444, f1 = 0.40000\n",
      "macro: prec = 0.33373, rec = 0.30955, f1 = 0.32119\n"
     ]
    }
   ],
   "source": [
    "# question 2\n",
    "evals = further_evaluate(te_labels, predictions, labels)\n",
    "pprint_evals(*evals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2\n",
    "**a)**\n",
    "\n",
    "Accuracy is a measure of how often our model is correct at predicting an instance's class. A high accuracy measure corresponds with a low FP and FN count, which means that the model gets a lot of predictions correct.\n",
    " \n",
    "Precision is a measure of how good a model is at predicting an instance to be interesting, when it is actually interesting. A high precision measure corresponds with a low FP count, meaning the model often does not predict that an instance should be interesting if it is not.\n",
    " \n",
    "Recall is a measure of how good our model is at not missing interesting instances. A high recall corresponds to a low FN count, meaning the model often does not predict an interesting instance as uninteresting.\n",
    " \n",
    "F-1 is the harmonic mean of recall and precision, giving a different repesentation to the overall correctness of the predictions our model makes. A high F-1 score corresponds with a low FP and FN count, which means that our model generally gets more predictions correct than not.\n",
    " \n",
    "Macro-averaging averages the recall and or precision over all classes, treating each class with the same weight. Micro-averaging takes class inbalances into account better, providing a more representative average based on the size of each class. \n",
    " \n",
    "For instance, the 'A' and 'A+' classes have relatively small sizes. As the 'A' and 'A+' classes have drastically different recall and precision metrics to the other classes, the macro-averages will dispropotionately contribute more weight to these classes, whereas the micro-average will provide a better, proportionally weighted, average of the measures.\n",
    " \n",
    "**b)**\n",
    "\n",
    "(Implementation above)\n",
    " \n",
    "All scores, precision, recall, and F-1 are typically below that of the accuracy. These values generally average around 30%.\n",
    " \n",
    "Generally, I have found, the model performs better (higher precision, recall, and F-1 scores) when the class size is larger. However, this tread is not monotonic. The 'F' class often attains the highest scores, however it is not the largest class (the 'C' class is). I believe that these classes must be more closely associated with features that have higher probabilities, as this would mean they are more frequently predicted, and generally more correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4\n",
    "class OneRClassifier:\n",
    "    def train(self, data, labels):\n",
    "        default_label = labels.mode()[0]\n",
    "        \n",
    "        best_rule, min_err = None, inf\n",
    "        for attr in data.columns:\n",
    "            rule = [(val, labels[data[attr] == val].mode()[0]) \n",
    "                              for val in data[attr].unique()]\n",
    "            err = sum(sum(labels[data[attr] == val] != label) for val, label in rule)\n",
    "            if err < min_err:\n",
    "                best_rule = (attr, rule)\n",
    "                min_err = err\n",
    "            \n",
    "        self.attr, rule = best_rule\n",
    "        self.rule_dict = dd(lambda: default_label, rule)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, instances):\n",
    "        predictions = pd.Series(index=instances.index, dtype='object')\n",
    "        for i in predictions.index:\n",
    "            predictions.loc[i] = self.rule_dict[instances.loc[i, self.attr]]\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "# class ZeroRClassifier:\n",
    "#     def train(self, data, labels):\n",
    "#         self.label = labels.mode()[0]\n",
    "        \n",
    "#         return self\n",
    "        \n",
    "#     def predict(self, instances):\n",
    "#         predictions = pd.Series(index=instances.index, dtype='object', \n",
    "#                                 data=[self.label] * instances.shape[0])\n",
    "\n",
    "#         return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2831\n",
      "   A+: prec = 0.00000, rec = 0.00000, f1 = 0.00000\n",
      "    A: prec = 0.00000, rec = 0.00000, f1 = 0.00000\n",
      "    B: prec = 0.20000, rec = 0.08475, f1 = 0.11905\n",
      "    C: prec = 0.25455, rec = 0.53846, f1 = 0.34568\n",
      "    D: prec = 0.33333, rec = 0.45000, f1 = 0.38298\n",
      "    F: prec = 0.00000, rec = 0.00000, f1 = 0.00000\n",
      "macro: prec = 0.13131, rec = 0.17887, f1 = 0.15145\n"
     ]
    }
   ],
   "source": [
    "# question 4\n",
    "model = OneRClassifier().train(tr, tr_labels)\n",
    "predictions = model.predict(te)\n",
    "\n",
    "accuracy = evaluate(te_labels, predictions)\n",
    "print(f\"accuracy: {accuracy:.4f}\")\n",
    "\n",
    "evals = further_evaluate(te_labels, predictions, labels)\n",
    "pprint_evals(*evals) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4\n",
    "**a)** \n",
    " \n",
    "I have chosen to use the One-R classification method.\n",
    " \n",
    "The One-R method is a simple decision-based classification method, effictively being a decision tree with a single decision node. In the training phase, the model chooses a single, simple rule to then use in its predictions. \n",
    "\n",
    "The rule is chosen as follows:\n",
    "\n",
    "    for each attribute:\n",
    "      for each value of the attribute:\n",
    "        rule: assign the most frequent class associated with the instances which \n",
    "            have this value for the attribute\n",
    "        calculate the error of assigning these instances with this \n",
    "            class, uning this rule\n",
    "    choose the rule which minimises the total error over each value\n",
    "    \n",
    "The prediction step is then as follows:\n",
    "\n",
    "    for each instance:\n",
    "      assign it the class which the rule assigns this instance's value for the \n",
    "          selected attribute\n",
    "    \n",
    "I chose this model for its general simplicity, as well as for it's use as a standard baseline in ML contexts\n",
    " \n",
    "**b)** \n",
    " \n",
    "(Implementation above)\n",
    " \n",
    "In my testing, I have found that the Naive Bayes Classifier outperformed the One R Classifier by approximately 5% in terms of accuracy, and generally far better in terms of (macro) recall, precision, and f-1 scores.\n",
    " \n",
    "The One-R classifier often classifies all instances as either 'D' or 'C', and rarely as 'B' and 'F', likely because there are more instances with these labels. As the 'D' and 'C' classes account for 54% of all instances, the classifier can never achieve an accuracy score higher than 54%. The One-R classifier also often choses to make it's rule based on the 'Fedu' feature. This is also why the macro scores are much bettwer with the Naive Bayes classifier, as many of the classes score 0.\n",
    " \n",
    "The increased performance of the NB classifier is likely due to it being able to predict some of the other 46% of instance's classes, while not predicting the 'C' and 'D' instances as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions (you may respond in a cell or cells below):\n",
    "\n",
    "You should respond to Question 1 and two additional questions of your choice. A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "### Question 1: Naive Bayes Concepts and Implementation\n",
    "\n",
    "- a Explain the ‘naive’ assumption underlying Naive Bayes. (1) Why is it necessary? (2) Why can it be problematic? Link your discussion to the features of the students data set. [no programming required]\n",
    "- b Implement the required functions to load the student dataset, and estimate a Naive Bayes model. Evaluate the resulting classifier using the hold-out strategy, and measure its performance using accuracy.\n",
    "- c What accuracy does your classifier achieve? Manually inspect a few instances for which your classifier made correct predictions, and some for which it predicted incorrectly, and discuss any patterns you can find.\n",
    "\n",
    "### Question 2: A Closer Look at Evaluation\n",
    "\n",
    "- a You learnt in the lectures that precision, recall and f-1 measure can provide a more holistic and realistic picture of the classifier performance. (i) Explain the intuition behind accuracy, precision, recall, and F1-measure, (ii) contrast their utility, and (iii) discuss the difference between micro and macro averaging in the context of the data set. [no programming required]\n",
    "- b Compute precision, recall and f-1 measure of your model’s predictions on the test data set (1) separately for each class, and (2) as a single number using macro-averaging. Compare the results against your accuracy scores from Question 1. In the context of the student dataset, and your response to question 2a analyze the additional knowledge you gained about your classifier performance.\n",
    "\n",
    "### Question 3: Training Strategies \n",
    "\n",
    "There are other evaluation strategies, which tend to be preferred over the hold-out strategy you implemented in Question 1.\n",
    "- a Select one such strategy, (i) describe how it works, and (ii) explain why it is preferable over hold-out evaluation. [no programming required]\n",
    "- b Implement your chosen strategy from Question 3a, and report the accuracy score(s) of your classifier under this strategy. Compare your outcomes against your accuracy score in Question 1, and explain your observations in the context of your response to question 3a.\n",
    "\n",
    "### Question 4: Model Comparison\n",
    "\n",
    "In order to understand whether a machine learning model is performing satisfactorily we typically compare its performance against alternative models. \n",
    "- a Choose one (simple) comparison model, explain (i) the workings of your chosen model, and (ii) why you chose this particular model. \n",
    "- b Implement your model of choice. How does the performance of the Naive Bayes classifier compare against your additional model? Explain your observations.\n",
    "\n",
    "### Question 5: Bias and Fairness in Student Success Prediction\n",
    "\n",
    "As machine learning practitioners, we should be aware of possible ethical considerations around the\n",
    "applications we develop. The classifier you developed in this assignment could for example be used\n",
    "to classify college applicants into admitted vs not-admitted – depending on their predicted\n",
    "grade.\n",
    "- a Discuss ethical problems which might arise in this application and lead to unfair treatment of the applicants. Link your discussion to the set of features provided in the students data set. [no programming required]\n",
    "- b Select ethically problematic features from the data set and remove them from the data set. Use your own judgment (there is no right or wrong), and document your decisions. Train your Naive Bayes classifier on the resulting data set containing only ‘unproblematic’ features. How does the performance change in comparison to the full classifier?\n",
    "- c The approach to fairness we have adopted is called “fairness through unawareness” – we simply deleted any questionable features from our data. Removing all problematic features does not guarantee a fair classifier. Can you think of reasons why removing problematic features is not enough? [no programming required]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
